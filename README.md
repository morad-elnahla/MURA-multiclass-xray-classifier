# ü¶¥ BONIFY ‚Äî AI-Powered Fracture & Abnormality Detection

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=flat-square&logo=python&logoColor=white)](https://python.org)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-FF6F00?style=flat-square&logo=tensorflow&logoColor=white)](https://tensorflow.org)
[![Keras](https://img.shields.io/badge/Keras-DenseNet169-D00000?style=flat-square&logo=keras&logoColor=white)](https://keras.io)
[![Streamlit](https://img.shields.io/badge/Streamlit-App-FF4B4B?style=flat-square&logo=streamlit&logoColor=white)](https://streamlit.io)
[![License](https://img.shields.io/badge/License-MIT-22c55e?style=flat-square)](LICENSE)
[![Dataset](https://img.shields.io/badge/Dataset-MURA_v1.1-0ea5e9?style=flat-square&logo=kaggle&logoColor=white)](https://www.kaggle.com/datasets/cjinny/mura-v11)

<br>

**A deep learning system for automated bone fracture and abnormality detection**  
**from musculoskeletal X-ray images ‚Äî 14-class classification across 7 body regions**

</div>

---

## üìã Table of Contents

- [What We Built](#-what-we-built)
- [Project Workflow](#-project-workflow)
- [Results](#-results)
- [Dataset](#-dataset)
- [Model Architecture](#-model-architecture)
- [Training Strategy](#-training-strategy)
- [BONIFY App ‚Äî Deployment](#bonify-app--deployment)
- [Project Structure](#-project-structure)
- [References](#-references)

---

## üéØ What We Built

BONIFY is an end-to-end deep learning pipeline for **musculoskeletal X-ray classification**. The system goes beyond simple binary fracture detection ‚Äî it simultaneously **identifies the body region** and **determines whether it is normal or abnormal**, classifying each X-ray into one of **14 distinct classes**.

The project covers the full ML lifecycle:

- üìÇ **Data loading and preprocessing** ‚Äî built a robust `tf.data` pipeline with augmentation
- üîç **Exploratory data analysis** ‚Äî visualized class distributions, imbalances, and sample images
- üß† **Model design** ‚Äî built a custom head on top of DenseNet169 with focal loss for imbalanced classes
- üöÄ **3-phase training** ‚Äî progressive transfer learning from frozen base to full fine-tuning
- üìä **Comprehensive evaluation** ‚Äî confusion matrix, F1, Cohen's Kappa, Top-3 accuracy, per-class analysis
- üñ•Ô∏è **Streamlit deployment** ‚Äî a clean web app (BONIFY) for real-time X-ray inference

| Property | Value |
|----------|-------|
| **Task** | Multi-Class Classification ‚Äî 14 classes |
| **Model** | DenseNet169 ‚Äî Pretrained on ImageNet |
| **Training** | 3-Phase Transfer Learning + Fine-Tuning |
| **Loss** | Categorical Focal Loss (Œ≥=2.0, Œ±=0.25) |
| **Dataset** | MURA v1.1 ‚Äî Stanford ML Group |
| **Image Size** | 320 √ó 320 px |
| **Framework** | TensorFlow / Keras |
| **Deployment** | Streamlit Web App |

---

## üîÑ Project Workflow

```
Raw MURA Dataset
      ‚îÇ
      ‚ñº
 Data Loading & Preprocessing
 ‚îú‚îÄ‚îÄ glob scan of PNG files
 ‚îú‚îÄ‚îÄ Extracted body part from folder name (XR_*)
 ‚îú‚îÄ‚îÄ Extracted label from path (positive/negative)
 ‚îî‚îÄ‚îÄ Built train / val / test DataFrames
      ‚îÇ
      ‚ñº
 Exploratory Data Analysis
 ‚îú‚îÄ‚îÄ Class distribution (Normal vs Abnormal)
 ‚îú‚îÄ‚îÄ Images per body part
 ‚îî‚îÄ‚îÄ Sample X-rays ‚Äî Normal vs Abnormal
      ‚îÇ
      ‚ñº
 tf.data Pipeline
 ‚îú‚îÄ‚îÄ Image resize ‚Üí 320√ó320
 ‚îú‚îÄ‚îÄ Normalization ‚Üí [0.0, 1.0]
 ‚îú‚îÄ‚îÄ Augmentation (train only):
 ‚îÇ     flip ¬∑ brightness ¬∑ contrast ¬∑ saturation ¬∑ crop
 ‚îî‚îÄ‚îÄ Class weights for imbalance
      ‚îÇ
      ‚ñº
 DenseNet169 + Custom Head
 ‚îú‚îÄ‚îÄ Phase 1 ‚Äî Transfer Learning  (LR: 1e-4, frozen base)
 ‚îú‚îÄ‚îÄ Phase 2 ‚Äî Fine-Tuning        (LR: 5e-6, all layers)
 ‚îî‚îÄ‚îÄ Phase 3 ‚Äî Deep Fine-Tuning   (LR: 1e-6, all layers)
      ‚îÇ
      ‚ñº
 Evaluation
 ‚îú‚îÄ‚îÄ Confusion Matrix (14√ó14)
 ‚îú‚îÄ‚îÄ F1, Kappa, Precision, Recall
 ‚îú‚îÄ‚îÄ Per-class accuracy
 ‚îî‚îÄ‚îÄ Correct vs Wrong predictions
      ‚îÇ
      ‚ñº
 BONIFY Streamlit App
 ‚îî‚îÄ‚îÄ Upload X-ray ‚Üí Body part + Normal/Abnormal + Confidence
```

---

## üìä Results

### Overall Metrics ‚Äî Test Set (3,197 images)

| Metric | Score |
|--------|-------|
| **Top-1 Accuracy** | **78.26%** |
| **Top-3 Accuracy** | **98.65%** |
| **Cohen's Kappa** | **0.7641** |
| **F1 Score (Macro)** | **0.7744** |
| **F1 Score (Weighted)** | **0.7791** |

> Top-3 accuracy of **98.65%** means the correct class appears in the model's top 3 predictions for almost every image ‚Äî clinically very useful for shortlisting.

### Accuracy by Body Region

| Region | Accuracy | Test Samples |
|--------|----------|-------------|
| ü¶æ Elbow | **83.4%** | 465 |
| ‚åö Wrist | **83.0%** | 659 |
| ü¶¥ Humerus | **78.5%** | 288 |
| ‚úã Hand | **76.7%** | 460 |
| ü©ª Shoulder | **76.0%** | 563 |
| ‚òùÔ∏è Finger | **75.5%** | 461 |
| üí™ Forearm | **70.4%** | 301 |

### Top Confused Class Pairs

| Predicted As | True Class | Count |
|---|---|---|
| HAND_Normal | HAND_Abnormal | 83 |
| FINGER_Normal | FINGER_Abnormal | 76 |
| WRIST_Normal | WRIST_Abnormal | 71 |

> Most confusion is Normal‚ÜîAbnormal within the same region ‚Äî the model correctly identifies body parts but occasionally misses subtle pathology.

### Training Curves

![Training Curves](all_files,Report,EDA/training_curves.png)

The 3-phase strategy is clearly visible ‚Äî each phase transition causes a brief dip then recovery as the learning rate resets and more layers unfreeze.

### Evaluation Results

![Evaluation Results](all_files,Report,EDA/evaluation_results.png)

### Correct vs Wrong Predictions

![Correct vs Wrong](all_files,Report,EDA/correct_vs_wrong.png)

---

## üì¶ Dataset

**MURA v1.1 ‚Äî Musculoskeletal Radiographs**  
Stanford ML Group ¬∑ [Kaggle Dataset](https://www.kaggle.com/datasets/cjinny/mura-v11)

MURA is one of the largest publicly available musculoskeletal radiology datasets, containing X-ray studies of 7 upper extremity body parts labeled as normal or abnormal by board-certified radiologists.

| Split | Images |
|-------|--------|
| Train (85%) | ~36,808 |
| Validation (15% stratified) | ~6,496 |
| Test (MURA valid folder) | 3,197 |

### Class Mapping ‚Äî 14 Classes

| Label | Class | Label | Class |
|-------|-------|-------|-------|
| 0 | ELBOW_Normal | 1 | ELBOW_Abnormal |
| 2 | FINGER_Normal | 3 | FINGER_Abnormal |
| 4 | FOREARM_Normal | 5 | FOREARM_Abnormal |
| 6 | HAND_Normal | 7 | HAND_Abnormal |
| 8 | HUMERUS_Normal | 9 | HUMERUS_Abnormal |
| 10 | SHOULDER_Normal | 11 | SHOULDER_Abnormal |
| 12 | WRIST_Normal | 13 | WRIST_Abnormal |

### Exploratory Data Analysis

![EDA Overview](all_files,Report,EDA/eda_overview.png)

Key observations:
- **59.6%** Normal / **40.4%** Abnormal ‚Äî mild class imbalance addressed with `compute_class_weight(balanced)`
- WRIST (9,752) and SHOULDER (8,379) dominate the training distribution
- HUMERUS (1,272) and FOREARM (1,825) are underrepresented ‚Äî handled by class weighting

### Sample X-Ray Images ‚Äî Normal vs Abnormal

![Sample Images](all_files,Report,EDA/sample_images.png)

---

## üß† Model Architecture

We chose **DenseNet169** for its dense connectivity pattern ‚Äî each layer receives feature maps from all preceding layers, making it highly effective for medical imaging where subtle patterns matter.

```
Input (320√ó320√ó3)
    ‚îî‚îÄ‚îÄ DenseNet169 (pretrained ImageNet weights)
        ‚îî‚îÄ‚îÄ GlobalAveragePooling2D
            ‚îî‚îÄ‚îÄ BatchNormalization
                ‚îî‚îÄ‚îÄ Dense(512, activation='relu')
                    ‚îî‚îÄ‚îÄ Dropout(0.4)
                        ‚îî‚îÄ‚îÄ Dense(14, activation='softmax')
```

### Why Focal Loss?

Standard cross-entropy treats all examples equally. With MURA's class imbalance and the inherent difficulty of subtle fractures, we used **Categorical Focal Loss** which:
- Down-weights easy examples the model already classifies correctly
- Forces the model to focus on hard, ambiguous cases
- Significantly improves performance on minority classes

```python
def categorical_focal_loss(gamma=2.0, alpha=0.25):
    def focal_loss(y_true, y_pred):
        y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1.0 - K.epsilon())
        ce     = -y_true * tf.math.log(y_pred)
        weight = alpha * y_true * tf.pow(1.0 - y_pred, gamma)
        return tf.reduce_mean(tf.reduce_sum(weight * ce, axis=-1))
    return focal_loss
```

---

## üöÄ Training Strategy

Rather than training all layers from scratch or fine-tuning everything at once, we used a **3-phase progressive strategy** to maximize stability and performance:

### Phase 1 ‚Äî Transfer Learning
The DenseNet169 base is frozen. Only the custom classification head is trained. This lets the head learn to map DenseNet features to our 14 classes without corrupting the pretrained weights.

```
LR: 1e-4  |  Epochs: 10  |  Base: frozen  |  Head: trainable
```

### Phase 2 ‚Äî Fine-Tuning
All layers are unfrozen at a much lower learning rate. The entire network adapts together to the medical imaging domain.

```
LR: 5e-6  |  Epochs: 20  |  All layers: trainable  |  Augmentation: ON
```

### Phase 3 ‚Äî Deep Fine-Tuning
Final refinement pass at an even lower learning rate with class weights applied to push performance on underrepresented classes.

```
LR: 1e-6  |  Epochs: 15  |  Class weights: ON
```

**Callbacks used throughout:**
- `EarlyStopping` ‚Äî patience=5, monitors `val_loss`
- `ReduceLROnPlateau` ‚Äî factor=0.5, patience=3
- `ModelCheckpoint` ‚Äî saves best model per phase

**Data augmentation (training only):**
Random horizontal flip ¬∑ Random vertical flip ¬∑ Random brightness ¬±15% ¬∑ Random contrast [0.85‚Äì1.15] ¬∑ Random saturation [0.85‚Äì1.15] ¬∑ Random crop/pad

---

## BONIFY App ‚Äî Deployment

---

### üñ•Ô∏è BONIFY App ‚Äî Live Demo Screenshots

The BONIFY Streamlit interface provides a clean clinical-style dashboard for real-time inference.

### üîπ Upload & Prediction Interface

![BONIFY UI 1](app_image/2.png)

### üîπ Prediction Output & Confidence Scores

![BONIFY UI 2](app_image/3.png)



---

## üé• Project Demo Video

Watch the full walkthrough and deployment demo on LinkedIn:

üîó **Project Video:**  
https://www.linkedin.com/posts/morad-elnahla_deeplearning-medicalimaging-computervision-ugcPost-7432905481653866496-j2TK
---

## üìÅ Project Structure

```
MURA-multiclass-xray-classifier/
‚îÇ
‚îú‚îÄ‚îÄ all_files,Report,EDA/
‚îÇ   ‚îú‚îÄ‚îÄ app.py                      # BONIFY Streamlit app
‚îÇ   ‚îú‚îÄ‚îÄ mura-multi-class.ipynb      # Full training & evaluation notebook
‚îÇ   ‚îú‚îÄ‚îÄ test_predictions.csv        # Complete test set predictions
‚îÇ   ‚îú‚îÄ‚îÄ test_probs.npy              # Softmax probabilities  (3197 √ó 14)
‚îÇ   ‚îú‚îÄ‚îÄ test_preds.npy              # Predicted class indices (3197,)
‚îÇ   ‚îú‚îÄ‚îÄ true_labels.npy             # Ground truth labels     (3197,)
‚îÇ   ‚îú‚îÄ‚îÄ eda_overview.png            # EDA visualizations
‚îÇ   ‚îú‚îÄ‚îÄ training_curves.png         # 3-phase training history
‚îÇ   ‚îú‚îÄ‚îÄ evaluation_results.png      # Confusion matrix + metrics
‚îÇ   ‚îú‚îÄ‚îÄ sample_images.png           # Sample X-rays per class
‚îÇ   ‚îî‚îÄ‚îÄ correct_vs_wrong.png        # Prediction examples
‚îÇ
‚îú‚îÄ‚îÄ Data/                           # Dataset documentation (PDF)
‚îú‚îÄ‚îÄ Documentation/                  # Full model documentation (DOCX)
‚îú‚îÄ‚îÄ Refrences/                      # References & citations (PDF)
‚îú‚îÄ‚îÄ code/                           # Additional scripts
‚îî‚îÄ‚îÄ README.md
```

> **Note:** The trained model file `densenet169_multiclass_final.keras` (~157 MB) is not included in the repository due to size constraints. It can be reproduced by running `mura-multi-class.ipynb` on Kaggle with the MURA v1.1 dataset.

---

## üìö References

1. Rajpurkar, P. et al. (2018). MURA: Large Dataset for Abnormality Detection in Musculoskeletal Radiographs. *arXiv:1712.06957*
2. Huang, G. et al. (2017). Densely Connected Convolutional Networks (DenseNet). *CVPR 2017*
3. Lin, T.Y. et al. (2017). Focal Loss for Dense Object Detection. *ICCV 2017*
4. Standring, S. (2020). Gray's Anatomy: The Anatomical Basis of Clinical Practice. *42nd Edition, Elsevier*

Full references and citations available in [`Refrences/`](Refrences/)

---

> ‚ö†Ô∏è **Disclaimer:** This project is for **research and educational purposes only**. BONIFY is not a medical device and should not be used for clinical diagnosis. Always consult a qualified medical professional for radiological interpretation.

<div align="center">
<br>

Made with ‚ù§Ô∏è &nbsp;¬∑&nbsp; BONIFY v1.0 &nbsp;¬∑&nbsp; 2025

</div>
